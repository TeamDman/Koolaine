{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TeamD\\.conda\\envs\\koolaine\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)rocessor_config.json: 100%|██████████| 363/363 [00:00<?, ?B/s] \n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 536/536 [00:00<?, ?B/s] \n",
      "Downloading (…)tencepiece.bpe.model: 100%|██████████| 1.30M/1.30M [00:00<00:00, 12.1MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 4.01M/4.01M [00:00<00:00, 21.8MB/s]\n",
      "Downloading (…)in/added_tokens.json: 100%|██████████| 558/558 [00:00<?, ?B/s] \n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 747/747 [00:00<?, ?B/s] \n",
      "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot instantiate this tokenizer from a slow version. If it's based on sentencepiece, make sure you have sentencepiece installed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Repos\\Koolaine\\experiments\\ocr\\donut.ipynb Cell 1\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repos/Koolaine/experiments/ocr/donut.ipynb#W0sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_dataset\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repos/Koolaine/experiments/ocr/donut.ipynb#W0sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Repos/Koolaine/experiments/ocr/donut.ipynb#W0sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m processor \u001b[39m=\u001b[39m DonutProcessor\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mnaver-clova-ix/donut-base-finetuned-rvlcdip\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repos/Koolaine/experiments/ocr/donut.ipynb#W0sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m model \u001b[39m=\u001b[39m VisionEncoderDecoderModel\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mnaver-clova-ix/donut-base-finetuned-rvlcdip\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Repos/Koolaine/experiments/ocr/donut.ipynb#W0sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m device \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\koolaine\\lib\\site-packages\\transformers\\processing_utils.py:226\u001b[0m, in \u001b[0;36mProcessorMixin.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[39mif\u001b[39;00m token \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    224\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mtoken\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m token\n\u001b[1;32m--> 226\u001b[0m args \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_get_arguments_from_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    227\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(\u001b[39m*\u001b[39margs)\n",
      "File \u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\koolaine\\lib\\site-packages\\transformers\\processing_utils.py:270\u001b[0m, in \u001b[0;36mProcessorMixin._get_arguments_from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    268\u001b[0m         attribute_class \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(transformers_module, class_name)\n\u001b[1;32m--> 270\u001b[0m     args\u001b[39m.\u001b[39mappend(attribute_class\u001b[39m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs))\n\u001b[0;32m    271\u001b[0m \u001b[39mreturn\u001b[39;00m args\n",
      "File \u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\koolaine\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:751\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer_class \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    749\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTokenizer class \u001b[39m\u001b[39m{\u001b[39;00mtokenizer_class_candidate\u001b[39m}\u001b[39;00m\u001b[39m does not exist or is not currently imported.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    750\u001b[0m         )\n\u001b[1;32m--> 751\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer_class\u001b[39m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    753\u001b[0m \u001b[39m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[0;32m    754\u001b[0m \u001b[39m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[0;32m    755\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "File \u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\koolaine\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2042\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2039\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2040\u001b[0m         logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloading file \u001b[39m\u001b[39m{\u001b[39;00mfile_path\u001b[39m}\u001b[39;00m\u001b[39m from cache at \u001b[39m\u001b[39m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 2042\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_from_pretrained(\n\u001b[0;32m   2043\u001b[0m     resolved_vocab_files,\n\u001b[0;32m   2044\u001b[0m     pretrained_model_name_or_path,\n\u001b[0;32m   2045\u001b[0m     init_configuration,\n\u001b[0;32m   2046\u001b[0m     \u001b[39m*\u001b[39minit_inputs,\n\u001b[0;32m   2047\u001b[0m     token\u001b[39m=\u001b[39mtoken,\n\u001b[0;32m   2048\u001b[0m     cache_dir\u001b[39m=\u001b[39mcache_dir,\n\u001b[0;32m   2049\u001b[0m     local_files_only\u001b[39m=\u001b[39mlocal_files_only,\n\u001b[0;32m   2050\u001b[0m     _commit_hash\u001b[39m=\u001b[39mcommit_hash,\n\u001b[0;32m   2051\u001b[0m     _is_local\u001b[39m=\u001b[39mis_local,\n\u001b[0;32m   2052\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2053\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\koolaine\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2253\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2251\u001b[0m \u001b[39m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[0;32m   2252\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 2253\u001b[0m     tokenizer \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(\u001b[39m*\u001b[39minit_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39minit_kwargs)\n\u001b[0;32m   2254\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[0;32m   2255\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[0;32m   2256\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnable to load vocabulary from file. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2257\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2258\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\koolaine\\lib\\site-packages\\transformers\\models\\xlm_roberta\\tokenization_xlm_roberta_fast.py:155\u001b[0m, in \u001b[0;36mXLMRobertaTokenizerFast.__init__\u001b[1;34m(self, vocab_file, tokenizer_file, bos_token, eos_token, sep_token, cls_token, unk_token, pad_token, mask_token, **kwargs)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    140\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    141\u001b[0m     vocab_file\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    151\u001b[0m ):\n\u001b[0;32m    152\u001b[0m     \u001b[39m# Mask token behave like a normal word, i.e. include the space before it\u001b[39;00m\n\u001b[0;32m    153\u001b[0m     mask_token \u001b[39m=\u001b[39m AddedToken(mask_token, lstrip\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, rstrip\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(mask_token, \u001b[39mstr\u001b[39m) \u001b[39melse\u001b[39;00m mask_token\n\u001b[1;32m--> 155\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[0;32m    156\u001b[0m         vocab_file,\n\u001b[0;32m    157\u001b[0m         tokenizer_file\u001b[39m=\u001b[39mtokenizer_file,\n\u001b[0;32m    158\u001b[0m         bos_token\u001b[39m=\u001b[39mbos_token,\n\u001b[0;32m    159\u001b[0m         eos_token\u001b[39m=\u001b[39meos_token,\n\u001b[0;32m    160\u001b[0m         sep_token\u001b[39m=\u001b[39msep_token,\n\u001b[0;32m    161\u001b[0m         cls_token\u001b[39m=\u001b[39mcls_token,\n\u001b[0;32m    162\u001b[0m         unk_token\u001b[39m=\u001b[39munk_token,\n\u001b[0;32m    163\u001b[0m         pad_token\u001b[39m=\u001b[39mpad_token,\n\u001b[0;32m    164\u001b[0m         mask_token\u001b[39m=\u001b[39mmask_token,\n\u001b[0;32m    165\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    166\u001b[0m     )\n\u001b[0;32m    168\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_file \u001b[39m=\u001b[39m vocab_file\n",
      "File \u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\koolaine\\lib\\site-packages\\transformers\\tokenization_utils_fast.py:102\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     99\u001b[0m slow_to_fast \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mslow_to_fast\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    101\u001b[0m \u001b[39mif\u001b[39;00m from_slow \u001b[39mand\u001b[39;00m slow_tokenizer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mslow_tokenizer_class \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 102\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    103\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot instantiate this tokenizer from a slow version. If it\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms based on sentencepiece, make sure you \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    104\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhave sentencepiece installed.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    105\u001b[0m     )\n\u001b[0;32m    107\u001b[0m \u001b[39mif\u001b[39;00m tokenizer_object \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    108\u001b[0m     fast_tokenizer \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(tokenizer_object)\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot instantiate this tokenizer from a slow version. If it's based on sentencepiece, make sure you have sentencepiece installed."
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-rvlcdip\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-rvlcdip\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "# load document image\n",
    "dataset = load_dataset(\"hf-internal-testing/example-documents\", split=\"test\")\n",
    "image = dataset[1][\"image\"]\n",
    "\n",
    "# prepare decoder inputs\n",
    "task_prompt = \"<s_rvlcdip>\"\n",
    "decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
    "\n",
    "pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "outputs = model.generate(\n",
    "    pixel_values.to(device),\n",
    "    decoder_input_ids=decoder_input_ids.to(device),\n",
    "    max_length=model.decoder.config.max_position_embeddings,\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    eos_token_id=processor.tokenizer.eos_token_id,\n",
    "    use_cache=True,\n",
    "    bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "    return_dict_in_generate=True,\n",
    ")\n",
    "\n",
    "sequence = processor.batch_decode(outputs.sequences)[0]\n",
    "sequence = sequence.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n",
    "sequence = re.sub(r\"<.*?>\", \"\", sequence, count=1).strip()  # remove first task start token\n",
    "print(processor.token2json(sequence))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "koolaine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
