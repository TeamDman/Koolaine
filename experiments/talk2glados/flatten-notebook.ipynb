{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cells': [{'cell_type': 'code', 'execution_count': 1, 'metadata': {}, 'outputs': [{'name': 'stdout', 'output_type': 'stream', 'text': ['Requirement already satisfied: fastapi in c:\\\\users\\\\teamd\\\\.conda\\\\envs\\\\torch\\\\lib\\\\site-packages (0.104.0)\\n', 'Requirement already satisfied: uvicorn in c:\\\\users\\\\teamd\\\\.conda\\\\envs\\\\torch\\\\lib\\\\site-packages (0.23.2)\\n', 'Requirement already satisfied: nest-asyncio in c:\\\\users\\\\teamd\\\\.conda\\\\envs\\\\torch\\\\lib\\\\site-packages (1.5.6)\\n', 'Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in c:\\\\users\\\\teamd\\\\.conda\\\\envs\\\\torch\\\\lib\\\\site-packages (from fastapi) (1.10.13)\\n', 'Requirement already satisfied: anyio<4.0.0,>=3.7.1 in c:\\\\users\\\\teamd\\\\.conda\\\\envs\\\\torch\\\\lib\\\\site-packages (from fastapi) (3.7.1)\\n', 'Requirement already satisfied: typing-extensions>=4.8.0 in c:\\\\users\\\\teamd\\\\.conda\\\\envs\\\\torch\\\\lib\\\\site-packages (from fastapi) (4.8.0)\\n', 'Requirement already satisfied: starlette<0.28.0,>=0.27.0 in c:\\\\users\\\\teamd\\\\.conda\\\\envs\\\\torch\\\\lib\\\\site-packages (from fastapi) (0.27.0)\\n', 'Requirement already satisfied: h11>=0.8 in c:\\\\users\\\\teamd\\\\.conda\\\\envs\\\\torch\\\\lib\\\\site-packages (from uvicorn) (0.14.0)\\n', 'Requirement already satisfied: click>=7.0 in c:\\\\users\\\\teamd\\\\.conda\\\\envs\\\\torch\\\\lib\\\\site-packages (from uvicorn) (8.1.7)\\n', 'Requirement already satisfied: exceptiongroup in c:\\\\users\\\\teamd\\\\.conda\\\\envs\\\\torch\\\\lib\\\\site-packages (from anyio<4.0.0,>=3.7.1->fastapi) (1.1.3)\\n', 'Requirement already satisfied: idna>=2.8 in c:\\\\users\\\\teamd\\\\.conda\\\\envs\\\\torch\\\\lib\\\\site-packages (from anyio<4.0.0,>=3.7.1->fastapi) (3.4)\\n', 'Requirement already satisfied: sniffio>=1.1 in c:\\\\users\\\\teamd\\\\.conda\\\\envs\\\\torch\\\\lib\\\\site-packages (from anyio<4.0.0,>=3.7.1->fastapi) (1.3.0)\\n', 'Requirement already satisfied: colorama in c:\\\\users\\\\teamd\\\\.conda\\\\envs\\\\torch\\\\lib\\\\site-packages (from click>=7.0->uvicorn) (0.4.6)\\n', 'Note: you may need to restart the kernel to use updated packages.\\n']}], 'source': ['%pip install --quiet fastapi uvicorn nest-asyncio']}, {'cell_type': 'markdown', 'metadata': {}, 'source': ['> why is nest_asyncio required?\\n', '\\n', '#### ChatGPT\\n', '\\n', 'The `nest_asyncio` library is used to \"patch\" Python\\'s standard library asyncio event loop to allow nested use of `asyncio.run` and event loops. This is particularly useful in environments like Jupyter Notebook where the event loop is already running in the background to handle things like cell execution and various I/O operations.\\n', '\\n', 'In a standard Python script, you generally control the event loop yourself. When you execute an ASGI app using Uvicorn or any other ASGI server, the server takes control of the event loop to handle HTTP requests asynchronously.\\n', '\\n', 'In contrast, Jupyter Notebook already has an event loop running to manage its interactive environment. This can lead to issues when you try to run another event loop for your FastAPI application on top of it. By using `nest_asyncio`, you patch the existing event loop to support nesting, effectively allowing the FastAPI event loop to run inside the Jupyter event loop without conflicts.\\n', '\\n', \"Without `nest_asyncio`, you'd likely run into an error like:\\n\", '\\n', '```\\n', 'RuntimeError: This event loop is already running\\n', '```\\n', '\\n', 'This library resolves that issue, enabling you to run asynchronous code like FastAPI apps directly within Jupyter cells. Given your technical background, you can think of `nest_asyncio` as a workaround to make two event-driven systems coexist peacefully within the same Python process.']}, {'cell_type': 'code', 'execution_count': 2, 'metadata': {}, 'outputs': [], 'source': ['import nest_asyncio\\n', 'nest_asyncio.apply()']}, {'cell_type': 'code', 'execution_count': 3, 'metadata': {}, 'outputs': [{'name': 'stdout', 'output_type': 'stream', 'text': ['G:\\\\ml\\\\glados-tts-upstream\\n']}], 'source': ['%cd G:\\\\ml\\\\glados-tts-upstream']}, {'cell_type': 'code', 'execution_count': 4, 'metadata': {}, 'outputs': [{'name': 'stdout', 'output_type': 'stream', 'text': ['Initializing TTS Engine...\\n']}], 'source': ['# https://github.com/R2D2FISH/glados-tts\\n', 'from glados import tts_runner\\n', 'glados = tts_runner(False, True)']}, {'cell_type': 'code', 'execution_count': 5, 'metadata': {}, 'outputs': [{'name': 'stdout', 'output_type': 'stream', 'text': ['Forward Tacotron took 77.73995399475098ms\\n', 'HiFiGAN took 593.1084156036377ms\\n']}], 'source': ['glados.speak(\"Hello, world!\", True)']}, {'cell_type': 'code', 'execution_count': 6, 'metadata': {}, 'outputs': [], 'source': ['from fastapi import FastAPI, Request\\n', 'app = FastAPI()']}, {'cell_type': 'code', 'execution_count': 7, 'metadata': {}, 'outputs': [], 'source': ['@app.get(\"/\")\\n', 'def read_root():\\n', '    return {\"message\": \"Hello, World!\"}']}, {'cell_type': 'code', 'execution_count': 8, 'metadata': {}, 'outputs': [], 'source': ['@app.post(\"/say\")\\n', 'async def echo(request: Request):\\n', '    body = await request.body()\\n', '    text = body.decode(\"utf-8\")\\n', '    glados.speak(text, True)\\n', '    return {\"echo\": text}\\n']}, {'cell_type': 'code', 'execution_count': 10, 'metadata': {}, 'outputs': [{'name': 'stderr', 'output_type': 'stream', 'text': ['INFO:     Started server process [32372]\\n', 'INFO:     Waiting for application startup.\\n', 'INFO:     Application startup complete.\\n', 'INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\\n']}], 'source': ['def run_glados_server():\\n', '    import uvicorn\\n', '    # The host and port parameters are optional\\n', '    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\\n', '    # this gets weird because the output gets dumped in future cells, whatever\\n', '\\n', 'import threading\\n', 'glados_thread = threading.Thread(target=run_glados_server)\\n', 'glados_thread.start()']}, {'cell_type': 'code', 'execution_count': 11, 'metadata': {}, 'outputs': [{'name': 'stdout', 'output_type': 'stream', 'text': ['Forward Tacotron took 70.86491584777832ms\\n', 'HiFiGAN took 542.7937507629395ms\\n', 'INFO:     127.0.0.1:9666 - \"POST /say HTTP/1.1\" 200 OK\\n']}, {'data': {'text/plain': ['<Response [200]>']}, 'execution_count': 11, 'metadata': {}, 'output_type': 'execute_result'}], 'source': ['import requests\\n', 'requests.post(\"http://localhost:8000/say\", data=\"Hello, world!\")']}, {'cell_type': 'code', 'execution_count': 12, 'metadata': {}, 'outputs': [{'name': 'stdout', 'output_type': 'stream', 'text': ['hi\\n']}, {'name': 'stdout', 'output_type': 'stream', 'text': ['Forward Tacotron took 68.87030601501465ms\\n', 'HiFiGAN took 431.3807487487793ms\\n', 'INFO:     127.0.0.1:9672 - \"POST /say HTTP/1.1\" 200 OK\\n', 'Forward Tacotron took 58.20322036743164ms\\n', 'HiFiGAN took 407.4423313140869ms\\n', 'INFO:     127.0.0.1:9674 - \"POST /say HTTP/1.1\" 200 OK\\n']}], 'source': ['print(\"hi\")']}], 'metadata': {'kernelspec': {'display_name': 'torch', 'language': 'python', 'name': 'python3'}, 'language_info': {'codemirror_mode': {'name': 'ipython', 'version': 3}, 'file_extension': '.py', 'mimetype': 'text/x-python', 'name': 'python', 'nbconvert_exporter': 'python', 'pygments_lexer': 'ipython3', 'version': '3.10.8'}}, 'nbformat': 4, 'nbformat_minor': 2}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "data = json.load(open('server.ipynb'))\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pip install --quiet fastapi uvicorn nest-asyncio\n",
      "\n",
      "%%\n",
      "\n",
      "import nest_asyncio\n",
      "nest_asyncio.apply()\n",
      "\n",
      "%%\n",
      "\n",
      "%cd G:\\ml\\glados-tts-upstream\n",
      "\n",
      "%%\n",
      "\n",
      "# https://github.com/R2D2FISH/glados-tts\n",
      "from glados import tts_runner\n",
      "glados = tts_runner(False, True)\n",
      "\n",
      "%%\n",
      "\n",
      "glados.speak(\"Hello, world!\", True)\n",
      "\n",
      "%%\n",
      "\n",
      "from fastapi import FastAPI, Request\n",
      "app = FastAPI()\n",
      "\n",
      "%%\n",
      "\n",
      "@app.get(\"/\")\n",
      "def read_root():\n",
      "    return {\"message\": \"Hello, World!\"}\n",
      "\n",
      "%%\n",
      "\n",
      "@app.post(\"/say\")\n",
      "async def echo(request: Request):\n",
      "    body = await request.body()\n",
      "    text = body.decode(\"utf-8\")\n",
      "    glados.speak(text, True)\n",
      "    return {\"echo\": text}\n",
      "\n",
      "\n",
      "%%\n",
      "\n",
      "def run_glados_server():\n",
      "    import uvicorn\n",
      "    # The host and port parameters are optional\n",
      "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
      "    # this gets weird because the output gets dumped in future cells, whatever\n",
      "\n",
      "import threading\n",
      "glados_thread = threading.Thread(target=run_glados_server)\n",
      "glados_thread.start()\n",
      "\n",
      "%%\n",
      "\n",
      "import requests\n",
      "requests.post(\"http://localhost:8000/say\", data=\"Hello, world!\")\n",
      "\n",
      "%%\n",
      "\n",
      "print(\"hi\")\n"
     ]
    }
   ],
   "source": [
    "# extract the code and build a string where each code block is separated by %% on its own line\n",
    "code = '\\n\\n%%\\n\\n'.join([\"\".join(cell['source']) for cell in data['cells'] if cell['cell_type'] == 'code'])\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install git+https://github.com/openai/whisper.git soundfile"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
