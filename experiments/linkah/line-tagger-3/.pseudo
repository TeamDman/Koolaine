we are using a work pattern which requires us to store intermediate representations on disk for inspection

to prompt the llm, create two files: request.rest, and request.body.txt
request.rest is always the same and is used for vscode integration
```
POST http://127.0.0.1:8000/generate
Content-Type: text/plain

< request.body.txt
```

request.body.txt follows instruction format
```
### Instruction:

### Response:
````
then invoke the mentioned endpoint, extract from response resp["results"][0]["text"], and append to prompt to create generated.txt


for i,line in input.txt:
    create dir work/item-i with zero-padding based on #lines
        - this will hold the request.rest, request.body
    url, notes = split line on first comma
    prompt LLM to generate list of tags given url and notes
    encourage response to be structured, can seed response with beginning of json
    "output should be parse-able with python's ast.literal_eval() and nothing else"
    if response isn't valid syntax, prompt LLM again to make it valid
    log each step of the way, printing file paths to terminal makes it easy to jump in file explorer

